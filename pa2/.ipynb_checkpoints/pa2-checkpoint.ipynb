{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2: Classification\n",
    "\n",
    "### Instructor: Yuanyuan Shi\n",
    "\n",
    "### Teaching Assistants\n",
    "\n",
    "- Yuexin Bian, [yubian@ucsd.edu]\n",
    "- Tz-Ying Wu, [tzw001@ucsd.edu]\n",
    "\n",
    "## Instructions\n",
    "1. This assignment must be completed individually.  \n",
    "2. All solutions should be written in this notebook, you can use markdown cell if you are going to add solutions with regard to discussion or analysis\n",
    "3. This notebook contains skeleton code, which should not be modified\n",
    "4. You must run all cells in this notebook and then export it as a pdf. You must also submit this notebook as an .ipynb file.\n",
    "5. You must submit both files (.pdf and .ipynb) on Gradescope. You must mark each problem on Gradescope in the pdf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from utils.solver import Solver\n",
    "import cv2\n",
    "from glob import glob\n",
    "np.random.seed(0)\n",
    "# Comment it if you are not using mac\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "# use thie plot function to plot the training loss and test loss\n",
    "def plot(train_loss, test_loss):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "    ax.plot(train_loss, label='train_loss')\n",
    "    ax.plot(test_loss, label='test_loss')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m X_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((left_data, right_data), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m y_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mlen\u001b[39m(left_data),\u001b[38;5;241m1\u001b[39m)),np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(right_data),\u001b[38;5;241m1\u001b[39m))), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y_train.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y_test.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Load Data for binary classification\n",
    "left_data_files = glob('dataset/data/39/*.jpg')\n",
    "left_data = np.array([cv2.imread(f,cv2.IMREAD_GRAYSCALE).reshape(-1) for f in left_data_files])\n",
    "right_data_files = glob('dataset/data/38/*.jpg')\n",
    "right_data = np.array([cv2.imread(f,cv2.IMREAD_GRAYSCALE).reshape(-1) for f in right_data_files])\n",
    "X_data = np.concatenate((left_data, right_data), axis=0)\n",
    "y_data = np.concatenate((np.ones((len(left_data),1)),np.zeros((len(right_data),1))), axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=40)\n",
    "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')\n",
    "print(f'X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}')\n",
    "\n",
    "classes = ['Keep right', 'Keep left']\n",
    "num_classes = 2\n",
    "samples_per_class = 5\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].reshape(32, 32))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1. Binary Classification with Regression  (20 points)\n",
    "### In this part you will implement \n",
    "### 1. Linear regression model (10 points) \n",
    "### 2. Report MSE and accuracy for the train set and test set (5 points)\n",
    "### 3. discuss why linear regression be a poor idea for classification (5 points)\n",
    "### You are expected to achieve above 47% accuracy for the best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement linear regression with L2 regularization model\n",
    "class LinearRegression():\n",
    "    \"\"\"\n",
    "    Linear regression model: the learnable weight is vector $w$ \n",
    "    \"\"\"\n",
    "    def __init__(self, d, reg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d: int, dimension of the data\n",
    "            reg: float, Scalar giving L2 regularization strength.\n",
    "        \"\"\"\n",
    "        self.reg = reg\n",
    "        self.d = d\n",
    "        # TODO: initialize the weight vector and w0 --> self.w, self.w0\n",
    "        # self.w should be initialized from a Gaussian G(0,1)\n",
    "        # self.w0 should be initialized to be 0\n",
    "        self.w = np.random.rand((d,1))\n",
    "        self.w0 = 0\n",
    "        self.threshold = 0.5\n",
    "\n",
    "    \n",
    "    # TODO: Using the gradient descent to update self.w and self.w0\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: array (N, d) N is the number of data, d is the number of dimension\n",
    "            y: array (N, 1)\n",
    "        No return, just update self.w and self.w0\n",
    "        \"\"\"\n",
    "        N, d = X.shape\n",
    "        assert d == self.d\n",
    "        self.w = np.linalg.inv(np.dot(X.T,X))\n",
    "        self.w = np.dot(np.dot(self.w, X.T), y)\n",
    "        assert self.w.shape == (d, 1)\n",
    "        self.w0 = np.average(y)\n",
    "    \n",
    "    #TODO: compute the predicted y given X\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: array (N, d) N is the number of data, d is the number of dimension\n",
    "        Returns:\n",
    "            y: array (N, 1)\n",
    "        \"\"\"\n",
    "        N, d = X.shape\n",
    "        assert d == self.d\n",
    "        return np.dot(X, self.w)+self.w0\n",
    "\n",
    "\n",
    "\n",
    "    #TODO: compute mean squared loss given y_true and y_pred\n",
    "    def evaluate_loss(self, y_true,  y_pred):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: array (N,1) N is the number of labels\n",
    "            y_pred: array (N,1) N is the number of labels\n",
    "        Return:\n",
    "            loss: float, the MSEloss\n",
    "        \"\"\"\n",
    "        return np.average((y_true-y_pred)**2)\n",
    "\n",
    "    \n",
    "    #TODO: compute accuracy given given y_true and y_pred\n",
    "    # First you should process pred_y to be 0 or 1 given the threshold\n",
    "    # then compute the accuracy\n",
    "    def evaluate_acc(self, y_true, y_pred, threshold):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: array (N,1) N is the number of labels\n",
    "            y_pred: array (N,1) N is the number of labels\n",
    "            threshold: float, \n",
    "        Return:\n",
    "            accuracy: float, the accuracy\n",
    "        \"\"\"\n",
    "        y_pred01 = np.where(y_pred > self,threshold, 1, 0)\n",
    "        N = y_true.shape[0]\n",
    "        correct = 0\n",
    "        for i in range(N):\n",
    "            if y_pred01[i,0] == y_true[i,0]:\n",
    "                correct += 1\n",
    "        return correct / N       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: conduct experiments\n",
    "reg = 0.2 # TODO: choose the appropriate regularization term\n",
    "N, d = X_train.shape\n",
    "lr = LinearRegression(d=d, reg=reg)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_train = lr.predict(X_train)\n",
    "train_mse = lr.evaluate_loss(y_train, y_pred_train)\n",
    "#TODO: choose the appropriate threshold term\n",
    "threshold = 0.5\n",
    "train_acc = lr.evaluate_acc(y_train, y_pred_train, threshold=threshold) \n",
    "y_pred_test = lr.predict(X_test)\n",
    "test_mse = lr.evaluate_loss(y_test, y_pred_test)\n",
    "test_acc = lr.evaluate_acc(y_test, y_pred_test, threshold=threshold) \n",
    "print(f'Train mse: {train_mse}, Train accuracy: {train_acc}')\n",
    "print(f'Test mse: {test_mse}, Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2. Binary Classification with Logistic Regression (20 points)\n",
    "### In this part you will implement \n",
    "### 1. Logistic regression model (10 points)\n",
    "### 2. plot log loss vs. iterations for the train set and test set (5 points)\n",
    "### 3. report the log loss and accuracy (5 points)\n",
    "### You are expected to achieve above 90% accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement LogisticRegression with L2 regularization model\n",
    "class LogisticRegression(LinearRegression):\n",
    "    def __init__(self, d, lr, reg, maxite=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d: int, the number of dimension\n",
    "            maxite: int, the maximum iterations during training to find optimal weights\n",
    "            lr: float, learning rate\n",
    "            reg: float, L2 regularization term\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        self.maxite = maxite \n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        #TODO: initialize the weight vector and w0--> self.w, self.w0\n",
    "        # self.w should be initialized from a Gaussian G(0,1), self.w0 should be initialized to be zero\n",
    "\n",
    "    \n",
    "    \n",
    "    # TODO: for every epoch,\n",
    "    # update the weight vector and w0 (!!!) only using the training data\n",
    "    # record the log loss for train data and test data\n",
    "    def train(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train: array (N, d) training data\n",
    "            y_train: array (N, 1) training labels\n",
    "            X_train: array (N, d) training data\n",
    "            y_train: array (N, 1) training labels\n",
    "        Returns:\n",
    "            train_loss: array (maxite, 1) log loss of maxite iterations\n",
    "            test_loss: array (maxite, 1) log loss of maxite iterations\n",
    "        \"\"\"\n",
    "\n",
    "            \n",
    "    \n",
    "    #TODO: compute the prediction given X following the assignment equation\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: array (N, d) N is the number of data, d is the number of dimension\n",
    "        Returns:\n",
    "            y: array (N, 1)\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    #TODO: compute log loss given y_true and y_pred\n",
    "    def evaluate_loss(self, y_true,  y_pred):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: array (N,1) N is the number of labels\n",
    "            y_pred: array (N,1) N is the number of labels\n",
    "        Return:\n",
    "            loss: float, the log loss\n",
    "        \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: conduct experiments\n",
    "# You are free to change the learning_rate, max_iteration, regularization term parameters\n",
    "N, d = X_train.shape\n",
    "Logmodel = LogisticRegression(lr = 0.0001, maxite=500, reg = 1.0, d = d)\n",
    "train_loss, test_loss = Logmodel.train(X_train, y_train, X_test, y_test)\n",
    "plot(train_loss, test_loss)\n",
    "\n",
    "y_pred_train = Logmodel.predict(X_train)\n",
    "train_loss = Logmodel.evaluate_loss(y_train, y_pred_train)\n",
    "threshold = 0.5\n",
    "train_acc = Logmodel.evaluate_acc(y_train, y_pred_train, threshold=threshold) \n",
    "y_pred_test = Logmodel.predict(X_test)\n",
    "test_loss = Logmodel.evaluate_loss(y_test, y_pred_test)\n",
    "test_acc = Logmodel.evaluate_acc(y_test, y_pred_test, threshold=threshold) \n",
    "print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4. Softmax classification with MLP (35 points)\n",
    "### In this part you will implement \n",
    "### 1. Affine forward and backward function (4 points)\n",
    "### 2. ReLU activation function and relu backward pass (4 points)\n",
    "### 3. Softmax loss layer (4 points)\n",
    "### 4. build a twolayer neural networks (8 points)\n",
    "### Conduct experiments using two layer NNs (15 points)\n",
    "### You are expected to achieve 55% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for softmax classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data for softmax classification\n",
    "K = 10\n",
    "class_idx = list(range(30,40)) # the categories to predict\n",
    "data_files = [glob(f'dataset/data/{i}/*.jpg') for i in class_idx]\n",
    "print('The number of samples for different categories: ')\n",
    "for i in range(K):\n",
    "    if i < K-1:\n",
    "        print(f'K_{i}: {len(data_files[i])}', end=', ')\n",
    "    else: print(f'K_{i}: {len(data_files[i])}')\n",
    "\n",
    "number_of_data = [len(data_files[i]) for i in range(K)]\n",
    "y_data = np.repeat(np.arange(0, K), np.array(number_of_data)).reshape(-1, 1)\n",
    "X_data_tuple = []\n",
    "for k in range(K):\n",
    "    X_data_tuple.append(np.array([cv2.imread(f,cv2.IMREAD_GRAYSCALE).reshape(-1)/255.0 for f in data_files[k]]))\n",
    "X_data = np.concatenate(X_data_tuple, axis=0)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=40)\n",
    "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')\n",
    "print(f'X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}')\n",
    "\n",
    "\n",
    "classes = ['Beware of ice/snow', 'Wild animals crossing', 'End of all speed and passing limits', 'Turn right ahead', 'Turn left ahead', 'Ahead only', 'Go straight or right', 'Go straight or left', 'Keep right', 'Keep left']\n",
    "num_classes = K\n",
    "samples_per_class = 5\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].reshape(32, 32))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='orange'>**Task 3.1: Affine layer: foward pass (no for loops are allowed) (1 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: affine layer: no for loops are allowed\n",
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
    "    # will need to reshape the input into rows.                               #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='orange'>**Task 3.1: Affine layer: backward pass (no for loops are allowed) (3 points).**\n",
    "    \n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='orange'>**Task 3.2: ReLU activation: forward pass (no for loops are allowed)**\n",
    "    \n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='orange'>**Task 3.2: ReLU activation: backward pass (no for loops are allowed).**\n",
    "    \n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='orange'>**Task 3.3: Softmax loss layer (no for loops are allowed) (4 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score (probability) for the jth\n",
    "      class for the ith input before softmax operation.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the softmax_loss function,                              #\n",
    "    # including the forward and backward passes.                              #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "np.random.seed(5330)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='orange'>**Task 3.4: Two-layer network (no for loops are allowed in your implementation)**\n",
    "    \n",
    "<font size='4'>Complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network with ReLU nonlinearity and\n",
    "    softmax loss that uses a modular layer design. We assume an input dimension\n",
    "    of D, a hidden dimension of H, and perform classification over C classes.\n",
    "\n",
    "    The architecure should be affine - relu - affine - softmax.\n",
    "\n",
    "    Note that this class does not implement gradient descent; instead, it\n",
    "    will interact with a separate Solver object that is responsible for running\n",
    "    optimization.\n",
    "\n",
    "    The learnable parameters of the model are stored in the dictionary\n",
    "    self.params that maps parameter names to numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=32*32, hidden_dim=100, num_classes=10,\n",
    "                 weight_scale=1.0, reg=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: An integer giving the size of the input\n",
    "        - hidden_dim: An integer giving the size of the hidden layer\n",
    "        - num_classes: An integer giving the number of classes to classify\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
    "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
    "        # standard deviation equal to weight_scale, and biases should be           #\n",
    "        # initialized to zero. All weights and biases should be stored in the      #\n",
    "        # dictionary self.params, with first layer weights                         #\n",
    "        # and biases using the keys 'W1' and 'b1' and second layer                 #\n",
    "        # weights and biases using the keys 'W2' and 'b2'.                         #\n",
    "        ############################################################################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, C) giving classification scores, where\n",
    "          scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        scores = None\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
    "        # class scores for X and storing them in the scores variable.              #\n",
    "        ############################################################################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        # If y is None then we are in test mode so just return scores\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
    "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
    "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
    "        # self.params[k]. Don't forget to add L2 regularization!                   #\n",
    "        #                                                                          #\n",
    "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "        # automated tests, make sure that your L2 regularization includes a factor #\n",
    "        # of 0.5 to simplify the expression for the gradient.                      #\n",
    "        ############################################################################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "np.random.seed(5330)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "# Errors should be around e-7 or less\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.solver import Solver\n",
    "\n",
    "# TODO: You are free to seclect hidden number, regularization term\n",
    "model = TwoLayerNet(input_dim=32*32, hidden_dim=100, num_classes=10, reg=1.0)\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the test set.                                        #\n",
    "##############################################################################\n",
    "\n",
    "raise NotImplementedError\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.55] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78c8e1b80b1031a1e136770481d214899efde752997b0b94966a80ddc7ada738"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
